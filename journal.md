# Journal de bord du projet encadrÃ©

## Cour1 
- J'ai dÃ©couvert les bases de Linux et ses commandes fondamentales. Bien que ces opÃ©rations soient nouvelles pour moi, j'ai compris leur importance pour la suite du cours. Je dois encore m'entraÃ®ner Ã  la maison pour mieux les maÃ®triser.
 - 1.ficher,dossier,dossier parent(..),dossier personnel(~),racine(/),dossier courant(pwd)
 - 2.chemin absolu VS chemin relatif
 - 3.caracteres de replacement(*/?),ls,cd,cat,man..etc

ğŸ”§ DifficultÃ©s:Apprentissage des options man(difficile pour la comprehension des mots techniques francais)
ğŸ‘ J'utilise souvent man pour maÃ®triser l'expression professionnelle, mais je trouve plus efficace de mÃ©moriser directement le code.


## Cour2
- J'ai crÃ©Ã© un nouveau dÃ©pÃ´t sur GitHub
- J'ai clonÃ© le dÃ©pÃ´t en local avec git clone
- J'ai crÃ©Ã© le fichier journal.md pour suivre mon progrÃ¨s
- J'ai crÃ©Ã© le fichier .gitignore pour ignorer les fichiers inutiles
- J'ai appris Ã  utiliser les commandes git add, commit et push

ğŸ”§ DifficultÃ©s: Confusion entre les diffÃ©rents Ã©tats des fichiers (untracked, staged, committed)
  Erreurs de permission lors du premier git push
ğŸ‘ J'ai demandÃ© conseil Ã  mon prof et utilisÃ© ChatGPT pour m'aider, m'entraÃ®nant assidÃ»ment en dehors des cours jusqu'Ã  ce que je maÃ®trise parfaitement comment pull et pull le dÃ©pÃ´t.


## Cour3
- J'ai appris Ã  utiliser la commande wc pour compter les lignes, mots et caractÃ¨res dans un fichier.
- J'ai compris la diffÃ©rence entre > (redirection Ã©crasante) et >> (redirection ajout).
- J'ai dÃ©couvert l'utilisation du pipe (|) pour chaÃ®ner des commandes et transmettre le rÃ©sultat d'une commande Ã  une autre.
- J'ai Ã©galement explorÃ© grep pour rechercher des motifs dans des fichiers, et commencÃ© Ã  Ã©crire mes premiers scripts Bash.

ğŸ”§ DifficultÃ©s: Les instructions conditionnelles en Bash ont Ã©tÃ© expliquÃ©es trÃ¨s rapidement en cours. Je n'ai pas tout saisi immÃ©diatement. 
ğŸ‘ Je compte m'exercer en autonomie pour mieux les comprendre et les appliquer.

âš ï¸ cat ./2016/* | grep Location | wc -l(âœ…) VS grep "Location" *.ann|grep "2016" |wc -l(âŒ) VS grep "Location" ./2016/*.ann | wc -l(âœ…)


## Cour4
- Jâ€™ai compris comment utiliser les boucles for pour rÃ©pÃ©ter une mÃªme commande sur plusieurs annÃ©es (2016, 2017, 2018), notamment dans le script comptes.sh.
- Jâ€™ai dÃ©couvert la gestion des arguments dans les scripts ($1, $2, $#) et comment adapter le comportement du programme selon les paramÃ¨tres fournis.
â†’ Exemple : ./compte_par_type.sh 2017 Location pour compter uniquement les entitÃ©s "Location" de 2017.
- Jâ€™ai appris Ã  valider les arguments dâ€™entrÃ©e avec des conditions if et des tests comme [ $# -ne 2 ] ou [ ! -d "$annee" ], afin dâ€™Ã©viter les erreurs dâ€™exÃ©cution.
- Jâ€™ai aussi utilisÃ© la commande find pour parcourir les sous-dossiers (par exemple 2016/01, 2016/02, etc.) et traiter les fichiers .ann de maniÃ¨re rÃ©cursive.
- Jâ€™ai compris comment formater la sortie et crÃ©er des scripts plus complets comme compte_par_type_par_an.sh et compte_lieux.sh, capables de produire des classements automatiques des lieux.

ğŸ”§ DifficultÃ©s : Au dÃ©but, jâ€™avais du mal Ã  comprendre la syntaxe des conditions if en Bash et la diffÃ©rence entre les crochets [ ] et les variables $#, $0, etc.
   De plus, jâ€™ai rencontrÃ© une erreur avec grep ("Is a directory") Ã  cause de la structure des sous-dossiers.

ğŸ‘ Solutions et progrÃ¨s : AprÃ¨s le cours, jâ€™ai relu les diapositives et consultÃ© des ressources en ligne pour mieux comprendre la logique du if et la validation des arguments.
   Jâ€™ai Ã©galement corrigÃ© mon script en utilisant find Ã  la place de grep *.ann, ce qui mâ€™a permis de traiter correctement les fichiers dans tous les sous-dossiers.
   Je me sens maintenant beaucoup plus Ã  lâ€™aise avec les bases du Bash et la construction de scripts paramÃ©trÃ©s.

### Explication_du_code

<img width="1098" height="1044" alt="image" src="https://github.com/user-attachments/assets/7fc686f0-5acc-4334-a896-60951c818e33" />

- Ce script sert Ã  : ğŸ‘‰ lire chaque ligne dâ€™un fichier texte et vÃ©rifier si elle correspond Ã  une URL valide (commenÃ§ant par http:// ou https://).
- Ã€ la fin, il affiche combien de lignes sont valides et combien sont douteuses.

- VÃ©rifie quâ€™un seul argument (le nom du fichier) a Ã©tÃ© fourni, sinon le programme sâ€™arrÃªte.
Initialise les compteurs pour les lignes valides et non valides.
- Commence une boucle pour lire le fichier ligne par ligne.
- Affiche la ligne lue actuellement.
- VÃ©rifie si la ligne ressemble Ã  une URL (commenÃ§ant par http ou https).
- Si la ligne correspond, elle est considÃ©rÃ©e comme une URL valide.
- Sinon, la ligne est considÃ©rÃ©e comme douteuse (non valide).
- Fin de la boucle : lit toutes les lignes du fichier.
- Affiche le nombre dâ€™URLs valides et de lignes douteuses.

## Cour5
- Cette semaine, nous avons commencÃ© Ã  travailler sur le mini-projet, qui consiste Ã  Ã©crire un script Bash capable de lire un fichier contenant plusieurs URL, dâ€™en rÃ©cupÃ©rer le contenu et dâ€™en extraire des informations pour produire un tableau.
Câ€™est la premiÃ¨re fois que je travaille sur un script un peu long et structurÃ©, et jâ€™ai compris lâ€™importance de bien organiser le code pour Ã©viter les erreurs.
- Jâ€™ai appris Ã  utiliser curl et lynx, deux outils essentiels pour interagir avec le web depuis le terminal.
Au dÃ©but, jâ€™avais du mal Ã  faire la diffÃ©rence entre eux :
je pensais quâ€™ils faisaient Ã  peu prÃ¨s la mÃªme chose.
Maintenant, je comprends que curl sert Ã  rÃ©cupÃ©rer le code source ou Ã  envoyer des requÃªtes HTTP, alors que lynx est un navigateur en ligne de commande qui permet dâ€™obtenir uniquement le texte visible dâ€™une page web, sans les balises HTML.
Cela mâ€™a permis de mieux visualiser la structure des pages et de comprendre la logique dâ€™extraction des donnÃ©es.
- Jâ€™ai aussi progressÃ© dans la validation des arguments dans les scripts Bash.
Je sais maintenant utiliser des tests comme [ $# -ne 1 ] pour vÃ©rifier le nombre dâ€™arguments fournis, et [ ! -f "$1" ] pour vÃ©rifier si le fichier passÃ© en paramÃ¨tre existe rÃ©ellement.
Jâ€™ai mieux compris la signification de variables comme $0 (nom du script), $1 (premier argument) et $# (nombre dâ€™arguments).
Ces notions me paraissaient abstraites au dÃ©but, mais en les pratiquant dans le mini-projet, elles deviennent beaucoup plus claires.
- Jâ€™ai dÃ©couvert dans lâ€™Ã©diteur Kate une fonction trÃ¨s utile : lâ€™analyse automatique de script (ShellCheck).
Cet outil signale les erreurs, les mauvaises pratiques ou les variables non utilisÃ©es dans le code.
GrÃ¢ce Ã  lui, jâ€™ai pu corriger plusieurs petits dÃ©tails et rendre mon script plus propre et plus lisible.
- En parallÃ¨le, jâ€™ai commencÃ© Ã  revoir les bases de HTML, car nous devons comprendre la structure dâ€™une page web pour pouvoir extraire les bonnes informations.
Je me rends compte que je ne maÃ®trise pas encore bien ce langage, notamment les balises <head>, <body> ou <title>.
Je compte utiliser des ressources en ligne pour approfondir mes connaissances.
Plus jâ€™avance, plus je rÃ©alise que jâ€™ai encore beaucoup Ã  apprendre, mais cela me motive Ã  continuer.

ğŸ”§ DifficultÃ©s :
- Le cours sur curl et lynx allait un peu vite, et je nâ€™ai pas encore eu le temps de bien maÃ®triser toutes les options, surtout celles de curl comme -i, -o ou -I.
De plus, certains sites web dans la liste dâ€™URL ne sont plus accessibles, ce qui provoque des erreurs avec Lynx â€” mais cela mâ€™a aussi appris Ã  Ãªtre patiente et Ã  anticiper les cas oÃ¹ un site ne rÃ©pond pas.

ğŸ‘ Ressenti personnel :
- Je sens que je deviens plus autonome dans la comprÃ©hension des scripts Bash et des commandes Linux.
MÃªme si jâ€™ai encore des lacunes (notamment en HTML et en manipulation web), je suis fiÃ¨re de mes progrÃ¨s.
Chaque exercice me montre un nouveau pan du fonctionnement dâ€™Internet et du traitement automatique du texte.
Cette semaine mâ€™a donnÃ© envie de poursuivre mes efforts et de mieux maÃ®triser les outils du web et de la programmation.

## Cour6
- Cette semaine, nous avons commencÃ© par corriger et commenter les exercices de la semaine prÃ©cÃ©dente.
Jâ€™ai revu en dÃ©tail la commande curl et jâ€™ai mieux compris le rÃ´le de ses nombreuses options, notamment Ã  travers la ligne suivante :
â€œcurl -o tmp.txt -k -i -s -L -w "%{content_type}\n%{http_code}" ${line} > metadata.tmpâ€
Voici ce que jâ€™ai appris :
<img width="1086" height="339" alt="å›¾ç‰‡" src="https://github.com/user-attachments/assets/430ef64a-bb23-4d55-b1d0-1f8658b8aba6" />
Jâ€™ai ensuite appris Ã  extraire ces informations Ã  lâ€™aide de commandes Bash :
encodage=$(cat metadata.tmp | head -n 1 | grep -E -o "charset=.*" | cut -d= -f2)
response=$(cat metadata.tmp | tail -n 1)
La premiÃ¨re commande rÃ©cupÃ¨re le nom du jeu de caractÃ¨res (charset), et la deuxiÃ¨me rÃ©cupÃ¨re le code de rÃ©ponse HTTP.Cela mâ€™a permis de comprendre comment analyser pas Ã  pas les mÃ©tadonnÃ©es dâ€™une page web dans un script Bash.

- En parallÃ¨le, nous avons Ã©galement Ã©tudiÃ© la structure de base du langage HTML, avec les balises <head> et <body>.
Jâ€™ai appris Ã  transformer un tableau CSV en une table HTML simple Ã  lâ€™aide des balises <table>, <tr>, <th> et <td>.

ğŸ”§ DifficultÃ©s :
- Je trouve que certaines options des commandes (surtout celles de curl sont encore difficiles Ã  mÃ©moriser et Ã  distinguer.Elles sont trÃ¨s nombreuses et parfois je mâ€™y perds un peu.

ğŸ‘ Ressenti personnel :
- Je compte rÃ©organiser mes notes pour mieux classer les options importantes et leurs exemples dâ€™utilisation.
Cela mâ€™aidera Ã  devenir plus efficace et Ã  mieux comprendre la logique de chaque commande.
