# Journal de bord du projet encadrÃ©

## Cours 1 
- J'ai dÃ©couvert les bases de Linux et ses commandes fondamentales. Bien que ces opÃ©rations soient nouvelles pour moi, j'ai compris leur importance pour la suite du cours. Je dois encore m'entraÃ®ner Ã  la maison pour mieux les maÃ®triser.
 - 1.ficher,dossier,dossier parent(..),dossier personnel(~),racine(/),dossier courant(pwd)
 - 2.chemin absolu VS chemin relatif
 - 3.caracteres de replacement(*/?),ls,cd,cat,man..etc

ğŸ”§ DifficultÃ©s:Apprentissage des options man(difficile pour la comprehension des mots techniques francais)
ğŸ‘ J'utilise souvent man pour maÃ®triser l'expression professionnelle, mais je trouve plus efficace de mÃ©moriser directement le code.


## Cours 2
- J'ai crÃ©Ã© un nouveau dÃ©pÃ´t sur GitHub
- J'ai clonÃ© le dÃ©pÃ´t en local avec git clone
- J'ai crÃ©Ã© le fichier journal.md pour suivre mon progrÃ¨s
- J'ai crÃ©Ã© le fichier .gitignore pour ignorer les fichiers inutiles
- J'ai appris Ã  utiliser les commandes git add, commit et push

ğŸ”§ DifficultÃ©s: Confusion entre les diffÃ©rents Ã©tats des fichiers (untracked, staged, committed)
  Erreurs de permission lors du premier git push
ğŸ‘ J'ai demandÃ© conseil Ã  mon prof et utilisÃ© ChatGPT pour m'aider, m'entraÃ®nant assidÃ»ment en dehors des cours jusqu'Ã  ce que je maÃ®trise parfaitement comment pull et pull le dÃ©pÃ´t.


## Cours 3
- J'ai appris Ã  utiliser la commande wc pour compter les lignes, mots et caractÃ¨res dans un fichier.
- J'ai compris la diffÃ©rence entre > (redirection Ã©crasante) et >> (redirection ajout).
- J'ai dÃ©couvert l'utilisation du pipe (|) pour chaÃ®ner des commandes et transmettre le rÃ©sultat d'une commande Ã  une autre.
- J'ai Ã©galement explorÃ© grep pour rechercher des motifs dans des fichiers, et commencÃ© Ã  Ã©crire mes premiers scripts Bash.

ğŸ”§ DifficultÃ©s: Les instructions conditionnelles en Bash ont Ã©tÃ© expliquÃ©es trÃ¨s rapidement en cours. Je n'ai pas tout saisi immÃ©diatement. 
ğŸ‘ Je compte m'exercer en autonomie pour mieux les comprendre et les appliquer.

âš ï¸ cat ./2016/* | grep Location | wc -l(âœ…) VS grep "Location" *.ann|grep "2016" |wc -l(âŒ) VS grep "Location" ./2016/*.ann | wc -l(âœ…)


## Cours 4
- Jâ€™ai compris comment utiliser les boucles for pour rÃ©pÃ©ter une mÃªme commande sur plusieurs annÃ©es (2016, 2017, 2018), notamment dans le script comptes.sh.
- Jâ€™ai dÃ©couvert la gestion des arguments dans les scripts ($1, $2, $#) et comment adapter le comportement du programme selon les paramÃ¨tres fournis.
â†’ Exemple : ./compte_par_type.sh 2017 Location pour compter uniquement les entitÃ©s "Location" de 2017.
- Jâ€™ai appris Ã  valider les arguments dâ€™entrÃ©e avec des conditions if et des tests comme [ $# -ne 2 ] ou [ ! -d "$annee" ], afin dâ€™Ã©viter les erreurs dâ€™exÃ©cution.
- Jâ€™ai aussi utilisÃ© la commande find pour parcourir les sous-dossiers (par exemple 2016/01, 2016/02, etc.) et traiter les fichiers .ann de maniÃ¨re rÃ©cursive.
- Jâ€™ai compris comment formater la sortie et crÃ©er des scripts plus complets comme compte_par_type_par_an.sh et compte_lieux.sh, capables de produire des classements automatiques des lieux.

ğŸ”§ DifficultÃ©s : Au dÃ©but, jâ€™avais du mal Ã  comprendre la syntaxe des conditions if en Bash et la diffÃ©rence entre les crochets [ ] et les variables $#, $0, etc.
   De plus, jâ€™ai rencontrÃ© une erreur avec grep ("Is a directory") Ã  cause de la structure des sous-dossiers.

ğŸ‘ Solutions et progrÃ¨s : AprÃ¨s le cours, jâ€™ai relu les diapositives et consultÃ© des ressources en ligne pour mieux comprendre la logique du if et la validation des arguments.
   Jâ€™ai Ã©galement corrigÃ© mon script en utilisant find Ã  la place de grep *.ann, ce qui mâ€™a permis de traiter correctement les fichiers dans tous les sous-dossiers.
   Je me sens maintenant beaucoup plus Ã  lâ€™aise avec les bases du Bash et la construction de scripts paramÃ©trÃ©s.

### Explication_du_code

<img width="1098" height="1044" alt="image" src="https://github.com/user-attachments/assets/7fc686f0-5acc-4334-a896-60951c818e33" />

- Ce script sert Ã  : ğŸ‘‰ lire chaque ligne dâ€™un fichier texte et vÃ©rifier si elle correspond Ã  une URL valide (commenÃ§ant par http:// ou https://).
- Ã€ la fin, il affiche combien de lignes sont valides et combien sont douteuses.

- VÃ©rifie quâ€™un seul argument (le nom du fichier) a Ã©tÃ© fourni, sinon le programme sâ€™arrÃªte.
Initialise les compteurs pour les lignes valides et non valides.
- Commence une boucle pour lire le fichier ligne par ligne.
- Affiche la ligne lue actuellement.
- VÃ©rifie si la ligne ressemble Ã  une URL (commenÃ§ant par http ou https).
- Si la ligne correspond, elle est considÃ©rÃ©e comme une URL valide.
- Sinon, la ligne est considÃ©rÃ©e comme douteuse (non valide).
- Fin de la boucle : lit toutes les lignes du fichier.
- Affiche le nombre dâ€™URLs valides et de lignes douteuses.

## Cours 5
- Cette semaine, nous avons commencÃ© Ã  travailler sur le mini-projet, qui consiste Ã  Ã©crire un script Bash capable de lire un fichier contenant plusieurs URL, dâ€™en rÃ©cupÃ©rer le contenu et dâ€™en extraire des informations pour produire un tableau.
Câ€™est la premiÃ¨re fois que je travaille sur un script un peu long et structurÃ©, et jâ€™ai compris lâ€™importance de bien organiser le code pour Ã©viter les erreurs.
- Jâ€™ai appris Ã  utiliser curl et lynx, deux outils essentiels pour interagir avec le web depuis le terminal.
Au dÃ©but, jâ€™avais du mal Ã  faire la diffÃ©rence entre eux :
je pensais quâ€™ils faisaient Ã  peu prÃ¨s la mÃªme chose.
Maintenant, je comprends que curl sert Ã  rÃ©cupÃ©rer le code source ou Ã  envoyer des requÃªtes HTTP, alors que lynx est un navigateur en ligne de commande qui permet dâ€™obtenir uniquement le texte visible dâ€™une page web, sans les balises HTML.
Cela mâ€™a permis de mieux visualiser la structure des pages et de comprendre la logique dâ€™extraction des donnÃ©es.
- Jâ€™ai aussi progressÃ© dans la validation des arguments dans les scripts Bash.
Je sais maintenant utiliser des tests comme [ $# -ne 1 ] pour vÃ©rifier le nombre dâ€™arguments fournis, et [ ! -f "$1" ] pour vÃ©rifier si le fichier passÃ© en paramÃ¨tre existe rÃ©ellement.
Jâ€™ai mieux compris la signification de variables comme $0 (nom du script), $1 (premier argument) et $# (nombre dâ€™arguments).
Ces notions me paraissaient abstraites au dÃ©but, mais en les pratiquant dans le mini-projet, elles deviennent beaucoup plus claires.
- Jâ€™ai dÃ©couvert dans lâ€™Ã©diteur Kate une fonction trÃ¨s utile : lâ€™analyse automatique de script (ShellCheck).
Cet outil signale les erreurs, les mauvaises pratiques ou les variables non utilisÃ©es dans le code.
GrÃ¢ce Ã  lui, jâ€™ai pu corriger plusieurs petits dÃ©tails et rendre mon script plus propre et plus lisible.
- En parallÃ¨le, jâ€™ai commencÃ© Ã  revoir les bases de HTML, car nous devons comprendre la structure dâ€™une page web pour pouvoir extraire les bonnes informations.
Je me rends compte que je ne maÃ®trise pas encore bien ce langage, notamment les balises <head>, <body> ou <title>.
Je compte utiliser des ressources en ligne pour approfondir mes connaissances.
Plus jâ€™avance, plus je rÃ©alise que jâ€™ai encore beaucoup Ã  apprendre, mais cela me motive Ã  continuer.

ğŸ”§ DifficultÃ©s :
- Le cours sur curl et lynx allait un peu vite, et je nâ€™ai pas encore eu le temps de bien maÃ®triser toutes les options, surtout celles de curl comme -i, -o ou -I.
De plus, certains sites web dans la liste dâ€™URL ne sont plus accessibles, ce qui provoque des erreurs avec Lynx â€” mais cela mâ€™a aussi appris Ã  Ãªtre patiente et Ã  anticiper les cas oÃ¹ un site ne rÃ©pond pas.

ğŸ‘ Ressenti personnel :
- Je sens que je deviens plus autonome dans la comprÃ©hension des scripts Bash et des commandes Linux.
MÃªme si jâ€™ai encore des lacunes (notamment en HTML et en manipulation web), je suis fiÃ¨re de mes progrÃ¨s.
Chaque exercice me montre un nouveau pan du fonctionnement dâ€™Internet et du traitement automatique du texte.
Cette semaine mâ€™a donnÃ© envie de poursuivre mes efforts et de mieux maÃ®triser les outils du web et de la programmation.

## Cours 6
- Cette semaine, nous avons commencÃ© par corriger et commenter les exercices de la semaine prÃ©cÃ©dente.
Jâ€™ai revu en dÃ©tail la commande curl et jâ€™ai mieux compris le rÃ´le de ses nombreuses options, notamment Ã  travers la ligne suivante :
â€œcurl -o tmp.txt -k -i -s -L -w "%{content_type}\n%{http_code}" ${line} > metadata.tmpâ€
Voici ce que jâ€™ai appris :
<img width="1086" height="339" alt="å›¾ç‰‡" src="https://github.com/user-attachments/assets/430ef64a-bb23-4d55-b1d0-1f8658b8aba6" />
Jâ€™ai ensuite appris Ã  extraire ces informations Ã  lâ€™aide de commandes Bash :
encodage=$(cat metadata.tmp | head -n 1 | grep -E -o "charset=.*" | cut -d= -f2)
response=$(cat metadata.tmp | tail -n 1)
La premiÃ¨re commande rÃ©cupÃ¨re le nom du jeu de caractÃ¨res (charset), et la deuxiÃ¨me rÃ©cupÃ¨re le code de rÃ©ponse HTTP.Cela mâ€™a permis de comprendre comment analyser pas Ã  pas les mÃ©tadonnÃ©es dâ€™une page web dans un script Bash.

- En parallÃ¨le, nous avons Ã©galement Ã©tudiÃ© la structure de base du langage HTML, avec les balises head et body.
Jâ€™ai appris Ã  transformer un tableau CSV en une table HTML simple Ã  lâ€™aide des balises table, tr, th et td.

ğŸ”§ DifficultÃ©s :
- Je trouve que certaines options des commandes (surtout celles de curl sont encore difficiles Ã  mÃ©moriser et Ã  distinguer.Elles sont trÃ¨s nombreuses et parfois je mâ€™y perds un peu.

ğŸ‘ Ressenti personnel :
- Je compte rÃ©organiser mes notes pour mieux classer les options importantes et leurs exemples dâ€™utilisation.
Cela mâ€™aidera Ã  devenir plus efficace et Ã  mieux comprendre la logique de chaque commande.

## Cours 7
- Cette semaine, nous avons Ã©tudiÃ© les bases du HTML et du CSS.
Jâ€™ai appris Ã  crÃ©er une page web depuis zÃ©ro et Ã  la rendre plus esthÃ©tique Ã  lâ€™aide de diffÃ©rentes techniques de mise en forme.
Nous avons revu la structure fondamentale dâ€™une page, avec les balises html, head et body, et appris Ã  insÃ©rer du texte, des titres, des liens ou des images.

- Nous avons dÃ©couvert trois maniÃ¨res principales de styliser une page web :
1ï¸âƒ£ en utilisant les balises HTML comme b, i, em ou strong pour mettre en valeur du texte ;
2ï¸âƒ£ en ajoutant des styles internes dans la section head avec la balise style ;
3ï¸âƒ£ en crÃ©ant un fichier CSS externe, reliÃ© Ã  la page grÃ¢ce Ã  <link>, pour mieux sÃ©parer le contenu et la prÃ©sentation.

- Nous avons Ã©galement explorÃ© le framework Bulma, une bibliothÃ¨que CSS trÃ¨s complÃ¨te qui propose de nombreuses classes prÃªtes Ã  lâ€™emploi pour crÃ©er rapidement des boutons, des menus, des sections, etc.
Cependant, Bulma contient tellement de styles que jâ€™ai passÃ© beaucoup de temps Ã  chercher et tester les bonnes classes Ã  utiliser.
Câ€™est parfois un peu dÃ©routant, mais trÃ¨s intÃ©ressant Ã  manipuler.

- Enfin, nous avons appris Ã  utiliser GitHub Pages pour publier nos pages web en ligne.
Câ€™Ã©tait la premiÃ¨re fois que je mettais mon propre site en ligne via GitHub, et voir ma page sâ€™afficher sur Internet mâ€™a donnÃ© une vraie satisfaction.

ğŸ”§ DifficultÃ©s :
Lors de la rÃ©daction du code HTML, jâ€™ai trouvÃ© que la gestion des niveaux et des balises imbriquÃ©es Ã©tait particuliÃ¨rement difficile.
Il faut toujours vÃ©rifier les ouvertures et les fermetures de balises, et il mâ€™est souvent arrivÃ© de devoir corriger plusieurs fois la structure du code.
Câ€™est un travail qui demande rigueur et patience.

ğŸ‘ Ressenti personnel :
Ce cours mâ€™a permis de mieux comprendre la logique du dÃ©veloppement web et la complÃ©mentaritÃ© entre HTML et CSS.
MÃªme si la mise en page demande beaucoup dâ€™essais et dâ€™ajustements, câ€™est trÃ¨s gratifiant de voir son propre site devenir plus clair et plus esthÃ©tique.
Je souhaite continuer Ã  pratiquer et Ã  mieux maÃ®triser les outils comme Bulma pour concevoir des pages plus professionnelles Ã  lâ€™avenir.

## Cours 8
- Cette semaine, nous avons Ã©tudiÃ© la gestion dâ€™un dÃ©pÃ´t GitHub en travail collaboratif, ainsi que les erreurs les plus courantes lorsquâ€™on travaille Ã  plusieurs.
Le cours portait surtout sur la maniÃ¨re de maintenir la synchronisation entre le dÃ©pÃ´t local et le dÃ©pÃ´t distant, et sur les bonnes pratiques Ã  adopter lorsquâ€™un push ou un pull Ã©choue.

- Nous avons approfondi plusieurs commandes essentielles de Git :
git reset (annuler un commit ou revenir Ã  un Ã©tat antÃ©rieur)
git checkout (changer de branche, restaurer un fichier, consulter un ancien commit)
git stash (mettre de cÃ´tÃ© des modifications temporaires)
et la notion de HEAD, qui reprÃ©sente la position actuelle dans lâ€™historique du projet.

- La partie la plus difficile pour moi a Ã©tÃ© git checkout, car cette commande possÃ¨de plusieurs fonctions et je les confondais souvent.
AprÃ¨s avoir rÃ©visÃ© le cours et cherchÃ© des explications supplÃ©mentaires, je comprends maintenant clairement ses trois usages principaux :
restaurer un fichier pour annuler les modifications locales,
changer de branche,
explorer lâ€™Ã©tat dâ€™un commit antÃ©rieur.

Pendant le cours, tout cela restait assez abstrait, mais les exercices mâ€™ont vraiment aidÃ©e Ã  rÃ©viser et Ã  appliquer ce que jâ€™avais appris.
Je commence Ã  mieux interprÃ©ter les messages dâ€™erreur de Git, et je sais quelle commande utiliser selon la situation.

- Par exemple, lorsquâ€™un push Ã©choue parce que le dÃ©pÃ´t local et le dÃ©pÃ´t distant ne sont pas synchronisÃ©s, je sais dÃ©sormais suivre la procÃ©dure correcte :
git fetch pour voir les diffÃ©rences,
git reset HEAD~1 pour annuler le dernier commit local,
git stash pour sauvegarder mes modifications,
git pull pour rÃ©cupÃ©rer la version Ã  jour du dÃ©pÃ´t distant,
git stash pop pour rÃ©appliquer mes changements,
et utiliser rÃ©guliÃ¨rement git status pour vÃ©rifier lâ€™Ã©tat du projet.

- GrÃ¢ce Ã  ces exercices, je me sens beaucoup plus Ã  lâ€™aise avec Git.
Je ne me contente plus dâ€™exÃ©cuter les commandes mÃ©caniquement : je comprends dÃ©sormais leur logique et leur rÃ´le dans la gestion dâ€™un projet collaboratif.
MÃªme si Git reste parfois abstrait, la pratique mâ€™aide Ã  progresser de maniÃ¨re solide et rÃ©guliÃ¨re.

## Cours 9

- Cette semaine, nous avons appris Ã  crÃ©er et gÃ©rer des environnements virtuels Python, afin de sÃ©parer proprement les dÃ©pendances de diffÃ©rents projets.
Jâ€™ai dÃ©couvert lâ€™utilisation de venv, ainsi que les commandes pour activer (source bin/activate) et dÃ©sactiver (deactivate) un environnement virtuel.
Nous avons Ã©galement comparÃ© les outils pip et uv, et compris leurs diffÃ©rences en termes de rapiditÃ© et de gestion des paquets.

- Un autre Ã©lÃ©ment important du cours a Ã©tÃ© lâ€™utilisation de wordcloud_cli pour gÃ©nÃ©rer des nuages de mots.
Jâ€™ai appris Ã  personnaliser une wordcloud en modifiant : les couleurs, les dimensions, les masques, la police, et la liste de stopwords. Câ€™Ã©tait la premiÃ¨re fois que je gÃ©nÃ©rais une visualisation directement depuis le terminal, ce que jâ€™ai trouvÃ© Ã  la fois pratique et motivant.

- Nous avons ensuite rÃ©alisÃ© un exercice de tokenisation, en utilisant un environnement virtuel Python pour dÃ©couper un fichier texte en tokens.
Câ€™est une Ã©tape de base mais essentielle pour les analyses linguistiques automatisÃ©es.

- En fin de sÃ©ance, nous avons dÃ©couvert le logiciel de textomÃ©trie Trameur.
Jâ€™ai compris que, pour que ce type dâ€™outil fonctionne correctement, il faut au prÃ©alable structurer le texte, par exemple en XML ou en marquant explicitement les divisions textuelles.
Nous avons Ã©galement abordÃ© la loi hypergÃ©omÃ©trique, utilisÃ©e en textomÃ©trie. Cette partie mâ€™a semblÃ© assez abstraite, et je pense quâ€™il me faudra une rÃ©vision supplÃ©mentaire pour bien comprendre son application.

ğŸ”§ DifficultÃ©s : Les notions statistiques liÃ©es Ã  la loi hypergÃ©omÃ©trique restent complexes pour moi, et lâ€™exigence de structuration des textes dans Trameur demande une certaine rigueur. Je dois encore approfondir ces points.

ğŸ‘ Ressenti personnel : Le contenu de cette sÃ©ance Ã©tait dense mais trÃ¨s intÃ©ressant : environnement Python, visualisation, traitement automatique et textomÃ©trie.
Cela mâ€™a montrÃ© toute la chaÃ®ne des outils nÃ©cessaires pour analyser un texte de maniÃ¨re professionnelle.
MÃªme si je dois encore progresser en mathÃ©matiques et en Python, je me sens de mieux en mieux Ã©quipÃ©e pour aborder les prochaines Ã©tapes du cours.

## Cours10
- Cette sÃ©ance a Ã©tÃ© consacrÃ©e Ã  une prÃ©sentation dÃ©taillÃ©e de la structure gÃ©nÃ©rale de notre projet final.
Le professeur a expliquÃ© toutes les Ã©tapes que nous devrons rÃ©aliser :
choix du mot Ã  Ã©tudier ; constitution dâ€™un fichier de 50 URLs ; aspiration des pages avec wget/curl ; extraction du texte brut avec lynx ; filtrage des contextes par egrep ; crÃ©ation des tableaux (URL, code HTTP, encodage, occurrences, etc.) ; gÃ©nÃ©ration de nuages de mots ; crÃ©ation dâ€™un concordancier ; analyse sÃ©parÃ©e pour chaque langue puis conclusion globale.
AprÃ¨s cette prÃ©sentation, nous avons commencÃ© Ã  travailler concrÃ¨tement sur nos projets : choix du mot, prÃ©paration des dossiers, rÃ©daction du fichier dâ€™URLs, et rÃ©flexion sur les futures analyses.

- La principale difficultÃ© que jâ€™ai rencontrÃ©e concerne la formulation de lâ€™hypothÃ¨se.
Je comprends quâ€™une hypothÃ¨se doit : Ãªtre liÃ©e Ã  un phÃ©nomÃ¨ne linguistique ; constituer une prÃ©diction que lâ€™on pourra vÃ©rifier Ã  lâ€™aide du corpus ; guider les analyses (occurrences, cooccurrences, nuages de mots, concordancier, etc.). Cependant, au moment de la rÃ©diger, je trouve cela encore trÃ¨s abstrait.
Jâ€™ai du mal Ã  transformer une intuition linguistique en une hypothÃ¨se claire et testable. Cette difficultÃ© mâ€™a montrÃ© que je dois encore approfondir la logique mÃ©thodologique avant de dÃ©finir mon hypothÃ¨se finale.

- MalgrÃ© cette incertitude, cette sÃ©ance mâ€™a permis de mieux comprendre lâ€™ensemble du workflow du projet :
collecte â†’ nettoyage â†’ structuration â†’ extraction â†’ statistiques â†’ visualisation â†’ analyse.
En avanÃ§ant dans le projet, je rÃ©alise que lâ€™hypothÃ¨se est essentielle pour orienter lâ€™analyse et pour donner un sens aux rÃ©sultats que nous allons obtenir.

## Cours11
- Cette sÃ©ance a Ã©tÃ© consacrÃ©e Ã  lâ€™avancement du projet.
Le cours Ã©tait organisÃ© sous forme de travail autonome : chacun avanÃ§ait sur son projet, et nous pouvions poser des questions au professeur dÃ¨s que nous rencontrions un problÃ¨me.
Ce fonctionnement mâ€™a permis de clarifier plusieurs Ã©tapes techniques du projet.

- Jâ€™ai terminÃ© une premiÃ¨re version de la page tableaux, oÃ¹ jâ€™ai intÃ©grÃ© les informations principales : URL, code HTTP, encodage, nombre dâ€™occurrences, dump textuel, etc.
Cependant, au cours de cette Ã©tape, jâ€™ai rencontrÃ© plusieurs difficultÃ©s :
certaines pages ne se redirigent pas correctement ; dâ€™autres exigent une authentification ou des cookies, ce qui empÃªche lâ€™aspiration via wget/curl ; quelques URLs renvoient des pages vides ou illisibles, ce qui bloque les Ã©tapes suivantes.

- Ces problÃ¨mes mâ€™ont montrÃ© que lâ€™aspiration des pages nâ€™est pas aussi simple que dans les exemples du cours.
Il faut tester les URLs, vÃ©rifier leur accessibilitÃ© et adapter le script pour Ã©viter les erreurs.
Je vais donc continuer Ã  amÃ©liorer mon projet : sÃ©lectionner de nouvelles URLs plus fiables, corriger les pages problÃ©matiques et finaliser la construction des tableaux et des contextes.
